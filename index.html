<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="UrbanIng-V2X">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="KEYWORD1, KEYWORD2, KEYWORD3, machine learning, computer vision, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="FIRST_AUTHOR_NAME, SECOND_AUTHOR_NAME">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>UrbanIng-V2X</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            <!-- TODO: Replace with actual paper title -->
            <h5>Paper Title 1</h5>
            <!-- TODO: Replace with brief description -->
            <p>Brief description of the work and its main contribution.</p>
            <!-- TODO: Replace with venue and year -->
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <!-- TODO: Add more related works or remove extra items -->
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">UrbanIng-V2X: A Large-Scale Multi-Vehicle,
Multi-Infrastructure Dataset Across Multiple
Intersections for Cooperative Perception</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
<span class="author-block">
  <a href="#" target="_blank">Karthikeyan Chandra Sekaran</a><sup>1*</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Markus Geisler</a><sup>1*</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Dominik Rößle</a><sup>1*</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Adithya Mohan</a><sup>1</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Daniel Cremers</a><sup>2</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Wolfgang Utschick</a><sup>2</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Michael Botsch</a><sup>1</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Werner Huber</a><sup>1</sup>,</span>
<span class="author-block">
  <a href="#" target="_blank">Torsten Schön</a><sup>1</sup>
</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">Technische Hochschule Ingolstadt and Technical University of Munich<br>NeurIPS 2025</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution. Authors listed in alphabetical order.</small></span>
                    <span class="eql-cntrb"><small><br><sup>1</sup>Technische Hochschule Ingolstadt</small></span>
                    <span class="eql-cntrb"><small><br><sup>2</sup>Technical University of Munich</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/thi-ad/UrbanIng-V2X/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/pdf/2510.23478" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
    <!-- Teaser Video -->
<div style="text-align: center; margin: 20px 0;">
  <iframe width="100%" height="400" 
    src="https://www.youtube.com/embed/F5fXbZHl05A?si=CfmVq118tGo4vaK2" 
    title="UrbanIng-V2X Teaser Video"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen>
  </iframe>
</div>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        A visual overview of UrbanIng-V2X, showcasing synchronized multi-view perception across vehicles and infrastructure. Each frame illustrates 3D bounding-box annotations from cameras and LiDAR sensors, highlighting cooperative perception and object tracking at complex urban intersections in Ingolstadt, Germany.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Recent cooperative perception datasets have played a crucial role in advancing
smart mobility applications by enabling information exchange between intelligent agents, helping to overcome challenges such as occlusions and improving
overall scene understanding. While some existing real-world datasets incorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions, they are
typically limited to a single intersection or a single vehicle. A comprehensive
perception dataset featuring multiple connected vehicles and infrastructure sensors
across several intersections remains unavailable, limiting the benchmarking of
algorithms in diverse traffic environments. Consequently, overfitting can occur,
and models may demonstrate misleadingly high performance due to similar intersection layouts and traffic participant behavior. To address this gap, we introduce
UrbanIng-V2X, the first large-scale, multi-modal dataset supporting cooperative
perception involving vehicles and infrastructure sensors deployed across three
urban intersections in Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned and spatially calibrated sensor sequences, each lasting 20 seconds.
All sequences contain recordings from one of three intersections, involving two
vehicles and up to three infrastructure-mounted sensor poles operating in coordinated scenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted
RGB cameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12 infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with 3D
bounding boxes spanning 13 object classes, resulting in approximately 712k annotated instances across the dataset. We provide comprehensive evaluations using
state-of-the-art cooperative perception methods and publicly release the codebase,
dataset, HD map, and a digital twin of the complete data collection environment
via https://github.com/thi-ad/UrbanIng-V2X.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- ========================= -->
<!-- Table I -->
<!-- ========================= -->

<table border="1" cellspacing="0" cellpadding="6" style="width:100%; border-collapse:collapse; text-align:center; font-family:Arial, sans-serif; font-size:14px;">
  <caption style="caption-side:top; font-weight:bold; text-align:left; padding:8px 0;">
    Table 1: Comparison of real-world cooperative <b>V2X</b> datasets with the proposed UrbanIng-V2X dataset (I = Infrastructure, V = Vehicle). 
    <br><small>†Images are not published yet.</small>
  </caption>
  <thead style="background-color:#f0f0f0;">
    <tr>
      <th>Property</th>
      <th>V2V4Real [30]</th>
      <th>DAIR-V2X-C [32]</th>
      <th>V2X-Seq [33]</th>
      <th>TUMTraf-V2X [36]</th>
      <th>V2XReal [26]</th>
      <th>UrbanIng-V2X (ours)</th>
    </tr>
  </thead>
  <tbody>
    <tr><td><b>Year</b></td><td>2022</td><td>2022</td><td>2023</td><td>2024</td><td>2024</td><td>2025</td></tr>
    <tr><td><b>V2X</b></td><td>V2V</td><td>V2X</td><td>V2X</td><td>V2I</td><td>V2V&amp;I</td><td>V2V&amp;I</td></tr>
    <tr><td><b>Intersections</b></td><td>0</td><td>28</td><td>1</td><td>1</td><td>1</td><td>3</td></tr>
    <tr><td><b>Vehicles</b></td><td>2</td><td>1</td><td>2</td><td>1</td><td>2</td><td>2</td></tr>
    <tr><td><b>RGB Images</b></td><td>40k†</td><td>39k</td><td>15k</td><td>5k</td><td>171k</td><td>81.6k</td></tr>
    <tr><td><b>IR Images</b></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>38.8k</td></tr>
    <tr><td><b>LiDAR Frames</b></td><td>40k</td><td>39k</td><td>10.45k</td><td>29k</td><td>33k</td><td>27k</td></tr>
    <tr><td><b>3D Boxes</b></td><td>240k</td><td>464k</td><td>10.45k</td><td>29k</td><td>1.2M</td><td>712k</td></tr>
    <tr><td><b>Classes</b></td><td>5</td><td>10</td><td>9</td><td>10</td><td>13</td><td>13</td></tr>
    <tr><td><b>Digital Twin</b></td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>Av. worldwide</b></td><td>Yes</td><td>No</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>HD Maps</b></td><td>Yes</td><td>No</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>Attributes</b></td><td>No</td><td>No</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>Track IDs</b></td><td>No</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>Traffic Light</b></td><td>No</td><td>No</td><td>No</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
    <tr><td><b>Sensors (I | V)</b></td><td>0 | 18</td><td>2 | 13</td><td>2 | 13</td><td>5 | 14</td><td>8 | 12</td><td>10 | 16</td></tr>
    <tr><td><b>City</b></td><td>Ohio</td><td>Beijing</td><td>Beijing</td><td>Munich</td><td>N.A.</td><td>Ingolstadt</td></tr>
    <tr><td><b>Country</b></td><td>USA</td><td>China</td><td>China</td><td>Germany</td><td>N.A.</td><td>Germany</td></tr>
  </tbody>
</table>


<!-- ========================= -->
<!-- UrbanIng-V2X Dataset Info -->
<!-- ========================= -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">At a Glance</h2>
    <div class="content has-text-justified">
      <p>
        <b>UrbanIng-V2X</b> is a large-scale <b>cooperative perception dataset</b> captured at three intelligent urban intersections in Ingolstadt, Germany. 
        It enables research in <i>multi-vehicle perception</i>, <i>vehicle-to-infrastructure (V2I)</i>, and <i>vehicle-to-vehicle (V2V)</i> communication.
      </p>
      <ul>
        <li><b>Scenarios:</b> 34 coordinated sequences (~20 s each)</li>
        <li><b>Objects:</b> ~712k 3D annotated bounding boxes</li>
        <li><b>Object classes:</b> 13</li>
        <li><b>Annotation rate:</b> 10 Hz</li>
        <li><b>License:</b> CC BY-NC-ND 4.0 (non-commercial academic use)</li>
      </ul>
    </div>

    <figure class="image is-16by9">
      <img src="static/images/TitlePicture.png" alt="UrbanIng-V2X Overview"
           loading="lazy" style="width:100%;border-radius:10px;box-shadow:0 2px 12px rgba(0,0,0,0.25);" />
      <figcaption class="has-text-centered">
        Overview of the UrbanIng-V2X setup: 2 vehicles, 3 infrastructure intersections, 12 LiDARs, 17 thermal cameras, 12 RGB cameras.
      </figcaption>
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Dataset Composition</h2>
    <div class="content has-text-justified">
      <p>
        UrbanIng-V2X provides a synchronized and spatially calibrated multi-modal dataset with contributions from both mobile and fixed sensing nodes:
      </p>
      <ul>
        <li><b>Vehicles (2):</b> Each equipped with 6 RGB cameras, 1 360° LiDAR, and 1 ADMA GNSS/IMU sensor.</li>
        <li><b>Infrastructure (up to 3 poles per intersection):</b> Each pole includes 4 LiDAR and multiple thermal cameras for 360° coverage.</li>
        <li><b>Collected environments:</b> Three distinct intersection layouts in Ingolstadt.</li>
        <li><b>Applications:</b> Cooperative object detection, tracking, trajectory prediction, and fusion evaluation.</li>
      </ul>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Calibration Overview</h2>
    <div class="content has-text-justified">
      <p>
        Each sensor within UrbanIng-V2X is temporally synchronized and spatially calibrated. 
        Calibration covers <b>LiDAR-to-camera</b>, <b>LiDAR-to-GNSS/IMU</b>, and <b>infrastructure alignment</b>. 
        The provided calibration files allow reconstruction of precise spatial relationships for fusion or cooperative perception tasks.
      </p>
    </div>

    <figure class="image is-16by9">
      <img src="static/images/calibration_fused_3.png" alt="Calibration overview"
           loading="lazy" style="width:100%;border-radius:10px;box-shadow:0 2px 12px rgba(0,0,0,0.25);" />
      <figcaption class="has-text-centered">
        Temporal and spatial calibration overview for vehicle and infrastructure sensors.
      </figcaption>
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Coordinate System</h2>
    <div class="content has-text-justified">
      <p>
        All data are expressed in a unified global coordinate system (GC) anchored to the ADMA GNSS/IMU reference frame. 
        Each vehicle’s local frame (x-forward, y-left, z-up) is registered to this global coordinate. 
        Infrastructure LiDARs and cameras are similarly aligned using surveyed extrinsics for interoperability.
      </p>
    </div>

    <figure class="image is-16by9">
      <img src="static/images/1.png" alt="Coordinate system alignment"
           loading="lazy" style="width:100%;border-radius:10px;box-shadow:0 2px 12px rgba(0,0,0,0.25);" />
      <figcaption class="has-text-centered">
        Coordinate system alignment between vehicles, LiDARs, and infrastructure units.
      </figcaption>
    </figure>
  </div>
</section>

<!-- BibTeX citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">Citation</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@misc{urbaningv2x2025,
  title={UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception},
  author={Karthikeyan Chandra Sekaran and Markus Geisler and Dominik Rößle and Adithya Mohan and Daniel Cremers and Wolfgang Utschick and Michael Botsch and Werner Huber and Torsten Schön},
  year={2025},
  eprint={2510.23478},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2510.23478}
}</code></pre>
  </div>
</section>

<!-- ========================= -->
<!-- Partner and Institute Logos -->
<!-- ========================= -->
<section class="section hero is-light">
  <div class="container has-text-centered">
    <h2 class="title is-4">In Collaboration With</h2>
    <div class="columns is-centered is-multiline is-vcentered" style="margin-top: 1rem;">
      
      <div class="column is-narrow">
        <figure class="image is-128x128" style="margin:auto;">
          <img src="static/images/aimotion_logo.png" alt="AI-Motion Bavaria" style="max-height:90px; object-fit:contain;">
        </figure>
      </div>

      <div class="column is-narrow">
        <figure class="image is-128x128" style="margin:auto;">
          <img src="static/images/carissma_logo.png" alt="CARISSMA Research Center" style="max-height:90px; object-fit:contain;">
        </figure>
      </div>

      <div class="column is-narrow">
        <figure class="image is-128x128" style="margin:auto;">
          <img src="static/images/cvims_logo.png" alt="CVIMS Research Group" style="max-height:90px; object-fit:contain;">
        </figure>
      </div>

      <div class="column is-narrow">
        <figure class="image is-128x128" style="margin:auto;">
          <img src="static/images/thi_logo.png" alt="Technische Hochschule Ingolstadt" style="max-height:90px; object-fit:contain;">
        </figure>
      </div>

      <div class="column is-narrow">
        <figure class="image is-128x128" style="margin:auto;">
          <img src="static/images/tum_logo.png" alt="Technical University of Munich" style="max-height:90px; object-fit:contain;">
        </figure>
      </div>

    </div>
  </div>
</section>




  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
